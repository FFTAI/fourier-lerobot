# GR00T Fine-tuning Preprocessing Script
This directory contains a utility script to convert a dataset generated by the `fourier-lerobot` converter into a format compatible with NVIDIA's GR00T model for fine-tuning.

The standard `fourier-lerobot` output stores task descriptions in a `meta/episodes.jsonl` file, with one entry per episode. However, GR00T's data loader expects task annotations to be present on a per-frame basis within the Parquet files. Additionally, GR00T requires these task annotations to be integer indices rather than raw strings.

This script automates the necessary transformation by:

**Generating a Task Map**: It first scans `meta/episodes.jsonl` to find all unique task descriptions and creates a `meta/tasks.jsonl` file, which maps each string-based task to a unique integer index.

**Broadcasting Annotations**: It iterates through every episode's Parquet file (e.g., `data/chunk-000/episode_000000.parquet`).

Applying Transformations: For each Parquet file, it adds a new column `annotation.tasks` to every frame. The value of this column is the integer index corresponding to the episode's task, making the dataset directly usable for GR00T.

## Prerequisites
- Python 3.8+

- Pandas (`pip install pandas`)

- tqdm (`pip install tqdm`)

- A dataset previously converted using the `scripts/convert_to_lerobot_v2.py` script from this repository.

## Usage
The script is designed to be run from the command line. You need to provide the paths to your dataset's `data` and `meta` directories.
```bash
python preprocess_for_gr00t.py --data_dir /path/to/your/output_dataset/data --meta_dir /path/to/your/output_dataset/meta
```
**Arguments**
- `--data_dir` **(required)**: The path to the directory containing the data chunks. This is typically the `data` folder inside your converted dataset directory, which contains subfolders like `chunk-000`, `chunk-001`, etc.

- `--meta_dir` **(required)**: The path to the directory containing the `episodes.jsonl` metadata file. This is typically the `meta` folder inside your converted dataset directory.

**Example**
Let's say your converted dataset has the following structure:
```plaintxt
my_fourier_dataset/
├── data/
│   ├── chunk-000/
│   │   ├── episode_000000.parquet
│   │   ├── episode_000001.parquet
│   │   └── ...
│   └── chunk-001/
│       └── ...
└── meta/
    └── episodes.jsonl
```
You would run the script like this:
```bash
python GR00T_finetune_transform/preprocess_for_gr00t.py \
    --data_dir my_fourier_dataset/data \
    --meta_dir my_fourier_dataset/meta
```
**What Happens After Running the Script?**
A new file, `my_fourier_dataset/meta/tasks.jsonl`, will be created. It will look something like this:
```json
{"task_index": 0, "task": "pick_up_the_red_block"}
{"task_index": 1, "task": "open_the_drawer"}
...
```
All `episode_*.parquet` files inside the `data/` subdirectories will be modified in-place. Each file will now contain an additional column named `annotation.tasks`, where every row has an integer value corresponding to the task index from `tasks.jsonl`.

Your dataset is now ready for fine-tuning with GR00T.